!pip install pyLDAvis
!pip install gensim

import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
from IPython.display import HTML, display
import tabulate
import pandas as pd
import numpy as np
from PIL import Image
from wordcloud import WordCloud
import seaborn as sns
from gensim.models.coherencemodel import CoherenceModel
import pyLDAvis
import pyLDAvis.gensim_models
# from nltk.tokenize import word_tokenize, sent_tokenize
from nltk.corpus import stopwords 
from nltk.stem import WordNetLemmatizer, PorterStemmer
# from nltk.stem.porter import *

import gensim
from gensim.models import Phrases
#Prepare objects for LDA gensim implementation
from gensim import corpora
#Running LDA
from gensim import models

import warnings
warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt
import re
%matplotlib inline
import string

data=pd.read_csv('/content/data/glassdoortest1.csv', engine='python')
data1 = data.replace('#NAME?',' ')
# dropping empty rows
data2 = data1.dropna(subset=['pros'])
# dropping duplicates
Data= data1.drop_duplicates()
print('Rows: {}, columns: {}'.format(Data.shape[0], Data.shape[1]))
Data.head(2)

Data['processed'] = ''
nltk.download('wordnet')
nltk.download('stopwords')

stop_words = stopwords.words('english')

stop_words.extend(['HTML'])
# initalizing the werdnet lemmatizer
lm = WordNetLemmatizer()

def processing(content):

   content =str(content).replace('\n', ' ').split(' ')
   #  removing these punctuations from tokens
   rx = re.compile('([&#.:?!-()])*')
   content = [rx.sub('', word) for word in content]
   #  removing stopwords
   content = [word.strip().lower() for word in content if word.strip().lower() not in stop_words]
   #  remove words whose length is greater than 1 and or alphabet only 
   content = [word for word in content if len(word)>1 and word.isalpha()]
   # lemmatizing the words to their basic form
   content = [lm.lemmatize(word) for word in content]
   return ' '.join(content)

for k in range(len(Data)):
  Data.iloc[k,-1] = processing(Data.iloc[k,3])
# processed data
Data.head()

b = Data['processed'].tolist()
b = ' '.join(map(str, b))
# b = b.replace(',', ' ').lower().replace('-', '')

wordcloud = WordCloud(max_font_size=50, max_words=1200, background_color="white", random_state=42,
                      prefer_horizontal=0.60).generate(b.lower())
# max_font_size=40, max_words=1000, background_color="white",
                      # random_state=100, prefer_horizontal=0.50
plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
# plt.show()
plt.savefig('word_cloud.jpg')

lengths = [len(sentence.split(' ')) if len(sentence)>0 else 0 for sentence in Data['processed']]
Data['Lengths'] = lengths
plt.figure(figsize=(10,6))
Data['Lengths'].hist(bins=20)
sns.despine(top=True, right=True, left=False, bottom=False)
plt.title('Number of words in each positive review(pros)')
plt.xlabel('Words')
plt.ylabel('Length of reviews')
plt.savefig('length_reviews.jpg')
plt.show()

import gensim.corpora as corpora

#decomposing sentences into tokens
tokens = [sentence.split(' ') for sentence in Data['processed'] ]
# training a bi gram model in order to include those bigrams as tokens who occured at least 6 times
bigram = gensim.models.Phrases(tokens, min_count=2, threshold=100)
bigram_mod = gensim.models.phrases.Phraser(bigram)

# including bigrams as tokens 
sents = [ bigram_mod[token] for token in tokens]

# Create Dictionary to keep track of vocab
Dict = corpora.Dictionary(tokens)

print('Unique number of words after pre-processing, before filtering', len(Dict))
# filter the words that occurs in less than 3 documents and in more than 50% of documents
Dict.filter_extremes(no_below= 3, no_above=0.50 )
print('Unique words after filtering', len(Dict))

# Create Corpus
corpus = [Dict.doc2bow(sent) for sent in sents]

tfidf = gensim.models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]

**LATENT DIRICHLET ALLOCATION**

%%time
from gensim.models import CoherenceModel
import time
import os

scores = []
for k in range(3,15):
    # LDA model
    lda_model = gensim.models.LdaModel(  corpus=corpus_tfidf, num_topics=k,
                                                 id2word=Dict, random_state=42)
    # to calculate score for coherence
    coherence_model_LDA = CoherenceModel(model=lda_model, texts=sents, dictionary=Dict, coherence='c_v')
    coherence_LDA = coherence_model_LDA.get_coherence()
    print("Topic",k,":", coherence_LDA)
    scores.append(coherence_LDA)

selected_topics = np.argmax(scores)+3

plt.figure(figsize=(14, 7))
plt.plot(list(range(3,15)), scores, marker='o', color='red')
sns.despine(top=True, right=True, left=False, bottom=False)

plt.locator_params(integer=True)
plt.title('Coherence score plotted against the number of topics for LDA')
plt.xlabel('Number of topics')
plt.ylabel('Coherence Scores')
plt.savefig('LDA_scores.jpg')
plt.show()

import pyLDAvis.gensim_models

LDA_model = gensim.models.LdaModel(corpus=corpus_tfidf, id2word=Dict, num_topics=selected_topics, 
                                           random_state=42, chunksize=128, passes=10 )

pyLDAvis.enable_notebook()
results = pyLDAvis.gensim_models.prepare(LDA_model, corpus_tfidf, Dict, sort_topics=False)
pyLDAvis.save_html(results, 'ldavis_english' +'.html')
results

# top words in each topic
LDA_model.print_topics()


**LATENT SEMANTIC ANALYSIS**

from gensim.models.lsimodel import LsiModel

scores_lsi = []
for k in range(3,15):
    # LSI model
    LSA_model = LsiModel(corpus=corpus_tfidf, num_topics=k,power_iters=250, id2word=Dict)
    # to calculate score for coherence
    coherence_model_LSA = CoherenceModel(model=LSA_model, texts=sents, dictionary=Dict, coherence='c_v')
    coherence_LSA = coherence_model_LSA.get_coherence()
    print("Topic", k,": ", coherence_LSA)
    scores_lsi.append(coherence_LSA)

plt.figure(figsize=(14, 7))
plt.plot(list(range(3,15)), scores_lsi, marker='o', color='blue')
sns.despine(top=True, right=True, left=False, bottom=False)

plt.locator_params(integer=True)
plt.title('Coherence score vs the number of topics for LSA')
plt.xlabel('Number of topics')
plt.ylabel('Coherence Scores')
plt.savefig('LSA_scores.jpg')
plt.show()

selected_topics_lsi = np.nanargmax(scores_lsi)+3
lsi_model = LsiModel(  corpus=corpus_tfidf, num_topics=selected_topics_lsi,
                                                 id2word=Dict)
lsi_model.print_topics()

# 1. Wordcloud of Top N words in each topic

from matplotlib import pyplot as plt
from wordcloud import WordCloud

cloud = WordCloud(background_color='white', width=2500, height=2000,
                  max_words=50, colormap='tab10', prefer_horizontal=1.0)
  
fig, axes = plt.subplots(1, 7, figsize=(17,8), sharex=True, sharey=True)

for i, ax in enumerate(axes.flatten()):
  
  fig.add_subplot(ax)
  if i>len(topics_name)-1:
    continue
  curr = Data[Data['Topics']==topics_name[i]]
  print(curr.shape)
  tokens = [tok for d in curr['processed'] for tok in d.split(' ')]
  cloud.generate(' '.join( tokens ))

  plt.gca().imshow(cloud)
  plt.gca().set_title( topics_name[i]+'\n')
  plt.gca().axis('off')

plt.axis('off')
plt.tight_layout()
plt.show()

def subcategory_plot(df, col):
    
    plt.figure(figsize=(20,8))
    bar_pub = list( df[col].value_counts().index[:8] )
    temp2 = df[df[col].isin(bar_pub)]

    sns.countplot(x=col, hue='Topics', data=temp2, palette="Accent_r")
    sns.despine()

    plt.savefig('bars_{}.png'.format(col))
    plt.show()

subcategory_plot(Data, 'title')

data=pd.read_csv('/content/data/glassdoortest1.csv', engine='python')
data1 = data.replace('#NAME?',' ')
# dropping empty rows
data2 = data1.dropna(subset=['cons'])
# dropping duplicates
Data= data1.drop_duplicates()
print('Rows: {}, columns: {}'.format(Data.shape[0], Data.shape[1]))
Data.head(2)

Data['processed'] = ''
nltk.download('wordnet')
nltk.download('stopwords')

stop_words = stopwords.words('english')

stop_words.extend(['HTML'])
# initalizing the werdnet lemmatizer
lm = WordNetLemmatizer()

def processing(content):

   content =str(content).replace('\n', ' ').split(' ')
   #  removing these punctuations from tokens
   rx = re.compile('([&#.:?!-()])*')
   content = [rx.sub('', word) for word in content]
   #  removing stopwords
   content = [word.strip().lower() for word in content if word.strip().lower() not in stop_words]
   #  remove words whose length is greater than 1 and or alphabet only 
   content = [word for word in content if len(word)>1 and word.isalpha()]
   # lemmatizing the words to their basic form
   content = [lm.lemmatize(word) for word in content]
   return ' '.join(content)

for k in range(len(Data)):
  Data.iloc[k,-1] = processing(Data.iloc[k,4])
# processed data
Data.head()

b = Data['processed'].tolist()
b = ' '.join(map(str, b))
# b = b.replace(',', ' ').lower().replace('-', '')

wordcloud = WordCloud(max_font_size=50, max_words=1200, background_color="white", random_state=42,
                      prefer_horizontal=0.60).generate(b.lower())
# max_font_size=40, max_words=1000, background_color="white",
                      # random_state=100, prefer_horizontal=0.50
plt.figure(figsize=(15,8))
plt.imshow(wordcloud, interpolation="bilinear")
plt.axis("off")
# plt.show()
plt.savefig('word_cloud.jpg')

lengths = [len(sentence.split(' ')) if len(sentence)>0 else 0 for sentence in Data['processed']]
Data['Lengths'] = lengths
plt.figure(figsize=(10,6))
Data['Lengths'].hist(bins=30)
sns.despine(top=True, right=True, left=False, bottom=False)
plt.title('Number of words in each negetive review(cons)')
plt.xlabel('Words')
plt.ylabel('Length of reviews')
plt.savefig('length_reviews.jpg')
plt.show()

import gensim.corpora as corpora

#decomposing sentences into tokens
tokens = [sentence.split(' ') for sentence in Data['processed'] ]
# training a bi gram model in order to include those bigrams as tokens who occured at least 6 times
# in the whole dataset
bigram = gensim.models.Phrases(tokens, min_count=2, threshold=100)
bigram_mod = gensim.models.phrases.Phraser(bigram)

# including bigrams as tokens 
sents = [ bigram_mod[token] for token in tokens]

# Create Dictionary to keep track of vocab
Dict = corpora.Dictionary(tokens)

print('Unique number of words after pre-processing, before filtering', len(Dict))
# no_below= 30
# filter the words that occurs in less than 3 documents and in more than 60% of documents
Dict.filter_extremes(no_below= 3, no_above=0.60 )
print('Unique words after filtering', len(Dict))

# Create Corpus
corpus = [Dict.doc2bow(sent) for sent in sents]

tfidf = gensim.models.TfidfModel(corpus)
corpus_tfidf = tfidf[corpus]

%%time
from gensim.models import CoherenceModel
import time
import os

scores = []
for k in range(3,15):
    # LDA model
    lda_model = gensim.models.LdaModel(  corpus=corpus_tfidf, num_topics=k,
                                                 id2word=Dict, random_state=42)
    # to calculate score for coherence
    coherence_model_LDA = CoherenceModel(model=lda_model, texts=sents, dictionary=Dict, coherence='c_v')
    coherence_LDA = coherence_model_LDA.get_coherence()
    print("Topic",k,":", coherence_LDA)
    scores.append(coherence_LDA)
    
    selected_topics = np.argmax(scores)+3

plt.figure(figsize=(14, 7))
plt.plot(list(range(3,15)), scores, marker='o', color='red')
sns.despine(top=True, right=True, left=False, bottom=False)

plt.locator_params(integer=True)
plt.title('Coherence score plotted against the number of topics for LDA')
plt.xlabel('Number of topics')
plt.ylabel('Coherence Scores')
plt.savefig('LDA_scores.jpg')
plt.show()

import pyLDAvis.gensim_models

LDA_model = gensim.models.LdaModel(corpus=corpus_tfidf, id2word=Dict, num_topics=selected_topics, 
                                           random_state=42, chunksize=128, passes=10 )

pyLDAvis.enable_notebook()
results = pyLDAvis.gensim_models.prepare(LDA_model, corpus_tfidf, Dict, sort_topics=False)
pyLDAvis.save_html(results, 'ldavis_english' +'.html')
results

# top words in each topic
LDA_model.print_topics()

**LATENT SEMANTIC ANALYSIS**

from gensim.models.lsimodel import LsiModel

scores_lsi = []
for k in range(3,15):
    # LSI model
    LSA_model = LsiModel(corpus=corpus_tfidf, num_topics=k,power_iters=250, id2word=Dict)
    # to calculate score for coherence
    coherence_model_LSA = CoherenceModel(model=LSA_model, texts=sents, dictionary=Dict, coherence='c_v')
    coherence_LSA = coherence_model_LSA.get_coherence()
    print("topic",k,":", coherence_LSA)
    scores_lsi.append(coherence_LSA)
    
    plt.figure(figsize=(14, 7))
plt.plot(list(range(3,15)), scores_lsi, marker='o', color='green')
sns.despine(top=True, right=True, left=False, bottom=False)

plt.locator_params(integer=True)
plt.title('Coherence score vs the number of topics for LSA')
plt.xlabel('Number of topics')
plt.ylabel('Coherence Scores')
plt.savefig('LSA_scores.jpg')
plt.show()

selected_topics_lsi = np.nanargmax(scores_lsi)+3
lsi_model = LsiModel(  corpus=corpus_tfidf, num_topics=selected_topics_lsi,
                                                 id2word=Dict)
lsi_model.print_topics()

# judged manually from pyldavis
topics_name = ['Topic 1', 'Topic 2', 'Topic 3', 'Topic 4', 'Topic 5', 'Topic 6', 'Topic 7', 'Topic 8']

# getting the most dominant topics from a trained model
predicted_topics = LDA_model[corpus_tfidf]

probs, topics = [], []
for k in predicted_topics:
  # sorting the probabilites
  k.sort(key=lambda x:x[1])
  # selecting the topic with greatest probability
  topics.append(topics_name[ k[0][0] ] )

Data['Topics'] = topics
Data.head(10)

# 1. Wordcloud of Top N words in each topic

from matplotlib import pyplot as plt
from wordcloud import WordCloud

cloud = WordCloud(background_color='white', width=2500, height=2000,
                  max_words=50, colormap='tab10', prefer_horizontal=1.0)
  
fig, axes = plt.subplots(1, 8, figsize=(17,8), sharex=True, sharey=True)

for i, ax in enumerate(axes.flatten()):
  
  fig.add_subplot(ax)
  if i>len(topics_name)-1:
    continue
  curr = Data[Data['Topics']==topics_name[i]]
  print(curr.shape)
  tokens = [tok for d in curr['processed'] for tok in d.split(' ')]
  cloud.generate(' '.join( tokens ))

  plt.gca().imshow(cloud)
  plt.gca().set_title( topics_name[i]+'\n')
  plt.gca().axis('off')

plt.axis('off')
plt.tight_layout()
plt.show()

def subcategory_plot(df, col):
    
    plt.figure(figsize=(20,8))
    bar_pub = list( df[col].value_counts().index[:8] )
    temp2 = df[df[col].isin(bar_pub)]

    sns.countplot(x=col, hue='Topics', data=temp2, palette="Accent_r")
    sns.despine()

    plt.savefig('bars_{}.png'.format(col))
    plt.show()

subcategory_plot(Data, 'title')
